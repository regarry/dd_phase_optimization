import torch
import torch.nn as nn
import torch.nn.functional as F

# AG This code was taken from
# ://debuggercafe.com/unet-from-scratch-using-pytorch/

def double_convolution_2d(in_channels, out_channels, dropout=0.0):
    """
    Convolution block with optional dropout.
    """
    conv_layers = [
        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
        nn.Dropout2d(dropout) if dropout > 0 else nn.Identity(),
        nn.LeakyReLU(inplace=True),
        nn.BatchNorm2d(out_channels),
        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
        nn.Dropout2d(dropout) if dropout > 0 else nn.Identity(),
        nn.LeakyReLU(inplace=True),
        nn.BatchNorm2d(out_channels)
    ]
    return nn.Sequential(*conv_layers)

def double_convolution_3d(in_channels, out_channels, dropout=0.0):
    """
    Convolution block with optional dropout.
    """
    kernel_size = (2, 3, 3)
    padding = 1
    conv_layers = [
        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),
        nn.Dropout3d(dropout) if dropout > 0 else nn.Identity(),
        nn.LeakyReLU(inplace=True),
        nn.BatchNorm3d(out_channels),
        nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),
        nn.Dropout3d(dropout) if dropout > 0 else nn.Identity(),
        nn.LeakyReLU(inplace=True),
        nn.BatchNorm3d(out_channels)
    ]
    return nn.Sequential(*conv_layers)


class OpticsDesignUnet(nn.Module):
    def __init__(self, config):
        super(OpticsDesignUnet, self).__init__()
        
        self.conv3d = config.get('conv3d', False)
        self.Nimgs = config.get('Nimgs', 1)
        num_classes = config['num_classes']
        dropout = config.get('dropout', 0.0)

        # --------------------------------------------------------
        # 2D U-NET ARCHITECTURE
        # --------------------------------------------------------
        if not self.conv3d:
            # Input: (Batch, Nimgs, H, W) -> Output: (Batch, Classes, H, W)
            self.norm = nn.BatchNorm2d(num_features=self.Nimgs, affine=True)
            self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

            # Contracting Path (Encoder)
            self.down_convolution_1 = double_convolution_2d(self.Nimgs, 64, dropout=dropout)
            self.down_convolution_2 = double_convolution_2d(64, 128, dropout=dropout)
            self.down_convolution_3 = double_convolution_2d(128, 256, dropout=dropout)

            # Expanding Path (Decoder)
            self.up_transpose_1 = nn.ConvTranspose2d(
                in_channels=256, out_channels=128,
                kernel_size=2, stride=2
            )
            self.up_convolution_1 = double_convolution_2d(256, 128, dropout=dropout)

            self.up_transpose_2 = nn.ConvTranspose2d(
                in_channels=128, out_channels=64,
                kernel_size=2, stride=2
            )
            self.up_convolution_2 = double_convolution_2d(128, 64, dropout=dropout)

            # Output Head
            self.out = nn.Conv2d(
                in_channels=64, out_channels=num_classes,
                kernel_size=1
            )

        # --------------------------------------------------------
        # 3D U-NET ARCHITECTURE
        # --------------------------------------------------------
        else:
            # Input: (Batch, 1, Depth, H, W) -> Output: (Batch, Classes, Depth, H, W)
            self.norm = nn.BatchNorm3d(num_features=1, affine=True)
            self.max_pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=2)

            # Contracting Path
            self.down_convolution_1 = double_convolution_3d(1, 64, dropout=dropout)
            self.down_convolution_2 = double_convolution_3d(64, 128, dropout=dropout)
            self.down_convolution_3 = double_convolution_3d(128, 256, dropout=dropout)

            # Expanding Path
            self.up_transpose_1 = nn.ConvTranspose3d(
                in_channels=256, out_channels=128,
                kernel_size=(1, 2, 2), stride=2
            )
            self.up_convolution_1 = double_convolution_3d(256, 128, dropout=dropout)

            self.up_transpose_2 = nn.ConvTranspose3d(
                in_channels=128, out_channels=64,
                kernel_size=(1, 2, 2), stride=2
            )
            self.up_convolution_2 = double_convolution_3d(128, 64, dropout=dropout)

            # Output Head
            self.out = nn.Conv3d(
                in_channels=64, out_channels=num_classes,
                kernel_size=1
            )

    def forward(self, x):
        """
        Pure Inference Forward Pass.
        Args:
            x (Tensor): The input image generated by the physics engine.
                        Shape: (Batch, Nimgs, H, W) if 2D
                        Shape: (Batch, 1, Depth, H, W) if 3D
        """
        # 1. Normalization
        # We assume 'x' is already on the correct device (handled by Wrapper)
        x = self.norm(x)

        # 2. Encoder
        down_1 = self.down_convolution_1(x)
        down_2 = self.max_pool(down_1)
        
        down_3 = self.down_convolution_2(down_2)
        down_4 = self.max_pool(down_3)
        
        down_5 = self.down_convolution_3(down_4)

        # 3. Decoder with Skip Connections
        up_1 = self.up_transpose_1(down_5)
        
        # Concatenate skip connection (down_3) with upsampled feature map
        x = self.up_convolution_1(torch.cat([down_3, up_1], dim=1))
        
        up_2 = self.up_transpose_2(x)
        
        # Concatenate skip connection (down_1)
        x = self.up_convolution_2(torch.cat([down_1, up_2], dim=1))

        # 4. Final Classification Layer
        out = self.out(x)
        
        return out

if __name__ == '__main__':
    input_image = torch.rand((1, 3, 512, 512))
    config = {'num_classes' : 1}
    model = OpticsDesignUnet(config)
    # Total parameters and trainable parameters.
    total_params = sum(p.numel() for p in model.parameters())
    print(f"{total_params:,} total parameters.")
    total_trainable_params = sum(
        p.numel() for p in model.parameters() if p.requires_grad)
    print(f"{total_trainable_params:,} training parameters.")
    outputs = model(input_image)
    print(outputs.shape)